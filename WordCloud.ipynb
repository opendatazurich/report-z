{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "from stop_words import get_stop_words\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from HanTa import HanoverTagger as ht\n",
    "\n",
    "import nltk\n",
    "\n",
    "#from nltk.tokenize import RegexpTokenizer\n",
    "#from nltk.stem.snowball import SnowballStemmer\n",
    "##nltk.download('wordnet')\n",
    "#from nltk.stem.wordnet import WordNetLemmatizer\n",
    "##nltk.download('stopwords')\n",
    "#from nltk.corpus import stopwords\n",
    "#from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ssz color palette to use correct colors for the City of Zurich\n",
    "import sszpalette\n",
    "sszpalette.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#force output to display the full description\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create connection to database\n",
    "conn = sqlite3.connect('data.sqlite')\n",
    "c = conn.cursor()\n",
    "\n",
    "#create the pandas data frame\n",
    "report_df = pd.read_sql('select signatur, titel, jahr, report_text from data', conn)\n",
    "\n",
    "#display the top records from the data frame\n",
    "report_df[['signatur', 'titel', 'jahr']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inline function to produce word count, splitting on spaces\n",
    "report_df['word_count'] = report_df['report_text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "report_df.word_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "stop_words = set(get_stop_words(\"de\", cache=False)) #show how many words are in the list of stop words\n",
    "\n",
    "# add some custom stopwords\n",
    "stop_words.update(['Jahr', 'Berichtsjahr', 'Fr', 'Po'])\n",
    "\n",
    "print(len(stop_words))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loops through descriptions and cleans them\n",
    "clean_text = []\n",
    "for w in range(len(report_df.report_text)):\n",
    "    text = report_df['report_text'][w].lower()\n",
    "    \n",
    "    #remove punctuation\n",
    "    text = re.sub('[^\\w]', ' ', text, flags=re.UNICODE)\n",
    "    \n",
    "    #remove digits and special chars\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \", text, flags=re.UNICODE)\n",
    "    \n",
    "    clean_text.append(text)#assign the cleaned descriptions to the data frame\n",
    "report_df['clean_text'] = clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texte analysieren\n",
    "\n",
    "Im nächsten Schritt wird jeder Text analysiert und die Nomen werden extrahier (weitere Möglichkeiten wären Stemming oder Lemmatizing).\n",
    "\n",
    "**ACHTUNG**: Der folgende Block dauert sehr lange über dem ganzen Korpus, Laufzeit ca. 4h!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nouns = []\n",
    "tagger = ht.HanoverTagger('morphmodel_ger.pgz')\n",
    "for w in tqdm(range(len(report_df['report_text'])), desc=\"Years\"):\n",
    "    sentences = nltk.sent_tokenize(report_df['report_text'][w], language='german')\n",
    "    \n",
    "    nouns = []\n",
    "    sentences_tok = [nltk.tokenize.word_tokenize(sent) for sent in sentences]\n",
    "    # try to lemmatize or stemming here instead of simply returning nouns\n",
    "    for sent in tqdm(sentences_tok, desc=\"Sentences\", colour=\"#03c2fc\", leave=False):\n",
    "        try:\n",
    "            tags = tagger.tag_sent(sent) \n",
    "            nouns_from_sent = [lemma for (word,lemma,pos) in tags if pos == \"NN\" or pos == \"NE\"]\n",
    "            # remove single character words\n",
    "            nouns.extend([n for n in nouns_from_sent if len(n) > 1])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    #print(report_df['jahr'][w], nouns[(len(nouns)//2-2):(len(nouns)//2+2)])\n",
    "    all_nouns.append(\" \".join(nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df['nouns'] = all_nouns\n",
    "report_df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed dataframe to disk\n",
    "#report_df.to_pickle(\"./report_processed.pkl\")\n",
    "report_df = pd.read_pickle(\"./report_processed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the frequency\n",
    "word_frequency = pd.Series(' '.join(report_df['nouns']).split()).value_counts()[:1000]\n",
    "word_frequency = word_frequency.to_dict()\n",
    "word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color='white', stopwords=stop_words, collocations=False)\n",
    "wc.generate(\" \".join(all_nouns))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from word_frequencies\n",
    "clean_freq = {k: v for k, v in word_frequency.items() if k not in stop_words}\n",
    "clean_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the word cloud parameters\n",
    "wordcloud = WordCloud(width=1800, height=1800, background_color='white', colormap='harmonic12', max_words=1000, min_font_size=20, min_word_length=2)\n",
    "wordcloud.generate_from_frequencies(clean_freq)\n",
    "#plot the word cloud\n",
    "fig = plt.figure(figsize = (20,15), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "#fig.savefig(\"wordcloud.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
